import json
import argparse
import glob
from pathlib import Path
from datetime import datetime
from typing import Literal, List, Dict, Any
from extractor import DocumentExtractor
from analyzer import DocumentAnalyzer
from models import DocumentAnalysis, InsightItem

# Importar exportador DOCX (opcional)
try:
    from docx_exporter import export_to_docx
    DOCX_EXPORT_AVAILABLE = True
except ImportError:
    DOCX_EXPORT_AVAILABLE = False


# Umbral mÃ­nimo de relevancia para incluir en el resumen de insights
DEFAULT_RELEVANCE_THRESHOLD = 0.5
# Filtro de tipo de insight: "all", "findings", "hypotheses", "actionable"
DEFAULT_INSIGHT_FILTER = "actionable"


def format_insight_text(insight: InsightItem, show_classification: bool = True) -> str:
    """Formatea un insight para mostrar en el resumen Markdown"""
    if show_classification:
        icons = {"finding": "ðŸ“Š", "hypothesis": "ðŸ’¡", "methodological_note": "ðŸ“"}
        labels = {"finding": "Hallazgo", "hypothesis": "HipÃ³tesis", "methodological_note": "Nota metodolÃ³gica"}
        icon = icons.get(insight.classification, "ðŸ’¡")
        classification_label = labels.get(insight.classification, "HipÃ³tesis")
        sample_info = f" (N={insight.sample_size})" if insight.sample_size else ""
        return f"{icon} **[{classification_label}]**{sample_info} {insight.text}"
    return insight.text


def filter_insights_by_type(insights: list, insight_filter: str) -> list:
    """Filtra insights segÃºn el tipo especificado
    
    Opciones:
    - 'all': Todos los insights (hallazgos + hipÃ³tesis + notas metodolÃ³gicas)
    - 'findings': Solo hallazgos cuantitativos
    - 'hypotheses': Solo hipÃ³tesis exploratorias
    - 'methodological_notes': Solo notas metodolÃ³gicas/descriptivas
    - 'actionable': Hallazgos + hipÃ³tesis (excluye notas metodolÃ³gicas)
    """
    if insight_filter == "all":
        return insights
    elif insight_filter == "findings":
        return [i for i in insights if i.classification == "finding"]
    elif insight_filter == "hypotheses":
        return [i for i in insights if i.classification == "hypothesis"]
    elif insight_filter == "methodological_notes":
        return [i for i in insights if i.classification == "methodological_note"]
    elif insight_filter == "actionable":
        return [i for i in insights if i.classification in ("finding", "hypothesis")]
    return insights


def save_ndjson(analysis: DocumentAnalysis, output_file: Path, source_file: str) -> Path:
    """
    Guarda el anÃ¡lisis en formato NDJSON con registros meta/claim/summary.
    Un JSON por lÃ­nea, sin markdown.
    """
    ndjson_file = output_file.with_suffix('.ndjson')
    
    # Recopilar todos los claims con IDs Ãºnicos
    claims = []
    claim_id = 1
    
    # Claims de grÃ¡ficos/imÃ¡genes
    for chart in analysis.chart_analysis:
        for insight in chart.insights:
            claims.append({
                "type": "claim",
                "id": f"C{claim_id:03d}",
                "page_number": None,  # No tenemos pÃ¡gina exacta para imÃ¡genes
                "source": "chart",
                "source_title": chart.title,
                "classification": insight.classification,
                "claim_text": insight.text,
                "evidence": {
                    "n": insight.sample_size,
                    "data_type": insight.evidence_type or "unknown",
                    "base_label": None
                },
                "theme_tags": insight.theme_tags,
                "ambiguity_flags": insight.ambiguity_flags,
                "classification_rationale": insight.classification_rationale,
                "relevance_score": chart.relevance_score
            })
            claim_id += 1
    
    # Claims del anÃ¡lisis de texto
    for td in analysis.text_data:
        if td.ai_analysis and td.ai_analysis.insights:
            for insight in td.ai_analysis.insights:
                claims.append({
                    "type": "claim",
                    "id": f"C{claim_id:03d}",
                    "page_number": td.page_number,
                    "source": "text",
                    "source_title": None,
                    "classification": insight.classification,
                    "claim_text": insight.text,
                    "evidence": {
                        "n": insight.sample_size,
                        "data_type": insight.evidence_type or "unknown",
                        "base_label": None
                    },
                    "theme_tags": insight.theme_tags,
                    "ambiguity_flags": insight.ambiguity_flags,
                    "classification_rationale": insight.classification_rationale,
                    "relevance_score": td.ai_analysis.relevance_score
                })
                claim_id += 1
    
    # Contar por clasificaciÃ³n
    counts = {
        "total_claims": len(claims),
        "findings": len([c for c in claims if c["classification"] == "finding"]),
        "hypotheses": len([c for c in claims if c["classification"] == "hypothesis"]),
        "methodological_notes": len([c for c in claims if c["classification"] == "methodological_note"])
    }
    
    # Identificar hipÃ³tesis prioritarias para validar
    top_hypotheses_to_validate = []
    for c in claims:
        if c["classification"] == "hypothesis" and not c["evidence"]["n"]:
            top_hypotheses_to_validate.append({
                "id": c["id"],
                "why": "Sin N especificado",
                "suggested_n": 100
            })
            if len(top_hypotheses_to_validate) >= 5:
                break
    
    # Registro meta
    meta_record = {
        "type": "meta",
        "study": {
            "study_name": analysis.metadata.study_name if analysis.metadata else analysis.filename,
            "source_file": source_file,
            "study_year": analysis.metadata.study_year if analysis.metadata else None,
            "company": analysis.metadata.company if analysis.metadata else None,
            "report_type": analysis.metadata.report_type if analysis.metadata else None,
            "report_date": None,
            "method": None,
            "sample_total_n": None
        },
        "extraction": {
            "extraction_date": analysis.extraction_date.isoformat(),
            "total_pages": analysis.total_pages,
            "charts_analyzed": len(analysis.chart_analysis),
            "images_extracted": len(analysis.image_data)
        }
    }
    
    # Registro summary
    summary_record = {
        "type": "summary",
        "counts": counts,
        "top_hypotheses_to_validate": top_hypotheses_to_validate,
        "method_limitations": []
    }
    
    # Escribir NDJSON
    with open(ndjson_file, 'w', encoding='utf-8') as f:
        # LÃ­nea 1: meta
        f.write(json.dumps(meta_record, ensure_ascii=False) + '\n')
        
        # LÃ­neas de claims
        for claim in claims:
            f.write(json.dumps(claim, ensure_ascii=False) + '\n')
        
        # Ãšltima lÃ­nea: summary
        f.write(json.dumps(summary_record, ensure_ascii=False) + '\n')
    
    return ndjson_file


def create_insights_summary(
    analysis: DocumentAnalysis, 
    output_file: Path, 
    relevance_threshold: float = DEFAULT_RELEVANCE_THRESHOLD,
    insight_filter: str = DEFAULT_INSIGHT_FILTER,
    show_classification: bool = True
) -> Path:
    """
    Crea un resumen breve de insights en formato Markdown.
    Solo incluye insights con relevance_score >= threshold.
    Filtra por tipo de insight segÃºn insight_filter.
    MÃ¡ximo 4 insights por grÃ¡fico y por pÃ¡gina de texto analizado.
    """
    insights_file = output_file.parent / f"insights-{output_file.stem.replace('_analysis', '')}.md"
    
    content = f"# Insights - {analysis.filename}\n\n"
    
    # Agregar metadata al inicio
    if analysis.metadata:
        content += "## Metadata del Documento\n\n"
        if analysis.metadata.study_year:
            content += f"**AÃ±o del estudio**: {analysis.metadata.study_year}  \n"
        if analysis.metadata.study_name:
            content += f"**Nombre**: {analysis.metadata.study_name}  \n"
        if analysis.metadata.company:
            content += f"**Empresa**: {analysis.metadata.company}  \n"
        if analysis.metadata.report_type:
            content += f"**Tipo**: {analysis.metadata.report_type}  \n"
        content += "\n"
    
    content += f"**Fecha de anÃ¡lisis**: {analysis.extraction_date.strftime('%Y-%m-%d %H:%M')}  \n"
    content += f"**Total pÃ¡ginas**: {analysis.total_pages} | **GrÃ¡ficos analizados**: {len(analysis.chart_analysis)}\n\n"
    
    # Mostrar filtros aplicados
    filter_label = {
        "all": "Todos", 
        "findings": "Solo Hallazgos", 
        "hypotheses": "Solo HipÃ³tesis",
        "methodological_notes": "Solo Notas metodolÃ³gicas",
        "actionable": "Hallazgos + HipÃ³tesis (sin notas metodolÃ³gicas)"
    }
    content += f"**Filtro**: {filter_label.get(insight_filter, insight_filter)} | **Umbral relevancia**: {relevance_threshold}\n\n"
    content += "---\n\n"
    
    has_insights = False
    total_findings = 0
    total_hypotheses = 0
    
    # Insights de grÃ¡ficos/imÃ¡genes (filtrados por relevancia)
    if analysis.chart_analysis:
        relevant_charts = [
            chart for chart in analysis.chart_analysis 
            if chart.insights and chart.relevance_score >= relevance_threshold
        ]
        
        # Filtrar charts que tengan insights del tipo deseado
        charts_with_filtered_insights = []
        for chart in relevant_charts:
            filtered = filter_insights_by_type(chart.insights, insight_filter)
            if filtered:
                charts_with_filtered_insights.append((chart, filtered))
        
        if charts_with_filtered_insights:
            has_insights = True
            content += "## Insights de GrÃ¡ficos\n\n"
            
            for chart_idx, (chart, filtered_insights) in enumerate(charts_with_filtered_insights, 1):
                content += f"### {chart_idx}. {chart.title or 'Sin tÃ­tulo'} ({chart.chart_data.type})\n\n"
                
                top_insights = filtered_insights[:4]
                for insight in top_insights:
                    content += f"- {format_insight_text(insight, show_classification)}\n"
                    if insight.classification == "finding":
                        total_findings += 1
                    elif insight.classification == "hypothesis":
                        total_hypotheses += 1
                    # observations no se cuentan en el resumen
                content += "\n"
    
    # Insights del anÃ¡lisis de texto con IA (filtrados por relevancia)
    text_pages_with_insights = [
        td for td in analysis.text_data 
        if td.ai_analysis and td.ai_analysis.insights and td.ai_analysis.relevance_score >= relevance_threshold
    ]
    
    # Filtrar pÃ¡ginas que tengan insights del tipo deseado
    pages_with_filtered_insights = []
    for td in text_pages_with_insights:
        filtered = filter_insights_by_type(td.ai_analysis.insights, insight_filter)
        if filtered:
            pages_with_filtered_insights.append((td, filtered))
    
    if pages_with_filtered_insights:
        has_insights = True
        content += "## Insights del AnÃ¡lisis de Texto\n\n"
        
        for text_data, filtered_insights in pages_with_filtered_insights:
            content += f"### PÃ¡gina {text_data.page_number}\n\n"
            
            top_insights = filtered_insights[:4]
            for insight in top_insights:
                content += f"- {format_insight_text(insight, show_classification)}\n"
                if insight.classification == "finding":
                    total_findings += 1
                elif insight.classification == "hypothesis":
                    total_hypotheses += 1
                # observations no se cuentan en el resumen
            content += "\n"
    
    # Resumen de clasificaciÃ³n
    if has_insights:
        content += "---\n\n"
        content += f"**Resumen**: {total_findings} hallazgos | {total_hypotheses} hipÃ³tesis\n\n"
    else:
        content += "_No se encontraron insights relevantes en el anÃ¡lisis._\n"
        content += f"\n_Umbral de relevancia: {relevance_threshold}_\n\n"
    
    # Leyenda de clasificaciÃ³n al final
    if show_classification:
        content += "---\n\n"
        content += "### Leyenda de ClasificaciÃ³n\n\n"
        content += "> ðŸ“Š **Hallazgo**: Respaldado por datos cuantitativos (N alto)  \n"
        content += "> ðŸ’¡ **HipÃ³tesis**: Exploratorio o cualitativo (requiere validaciÃ³n)  \n"
        content += "> ðŸ“ **Nota metodolÃ³gica**: DescripciÃ³n metodolÃ³gica/contextual\n"
    
    with open(insights_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return insights_file


def process_document(file_path: str, config_path: str = "config.json", domain_prompts_file: str = None, export_docx: bool = False) -> DocumentAnalysis:
    """
    Procesa un documento completo:
    1. Auto-convierte PPTX a PDF si es necesario
    2. Extrae texto e imÃ¡genes
    3. Analiza grÃ¡ficos con IA (Claude o OpenAI)
    4. Guarda resultados
    """
    original_path = file_path
    ext = Path(file_path).suffix.lower()
    temp_pdf_created = False
    
    # Auto-convertir PPTX a PDF
    if ext in ['.ppt', '.pptx']:
        print(f"\nðŸ”„ Detectado PowerPoint, convirtiendo a PDF...")
        from pptx_converter import convert_pptx_to_pdf
        
        # Cargar configuraciÃ³n para temp_dir
        with open(config_path, 'r') as f:
            config = json.load(f)
        temp_dir = Path(config.get('extraction', {}).get('pptx_conversion', {}).get('temp_dir', 'output/temp_pdfs'))
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            file_path = convert_pptx_to_pdf(file_path, str(temp_dir), config_path)
            temp_pdf_created = True
            print(f"   âœ“ Convertido a: {file_path}\n")
        except Exception as e:
            print(f"   âŒ Error en conversiÃ³n: {e}")
            raise
    
    print(f"\n{'='*60}")
    print(f"Procesando: {original_path}")
    if temp_pdf_created:
        print(f"(Convertido a: {Path(file_path).name})")
    print(f"{'='*60}\n")
    
    # Paso 1: ExtracciÃ³n
    print("ðŸ“„ Extrayendo contenido...")
    extractor = DocumentExtractor(config_path)
    text_data, image_data = extractor.extract(file_path)
    print(f"   âœ“ {len(text_data)} pÃ¡ginas de texto")
    print(f"   âœ“ {len(image_data)} imÃ¡genes")
    
    # Paso 2: AnÃ¡lisis
    print("\nðŸ” Analizando contenido...")
    analyzer = DocumentAnalyzer(config_path, domain_prompts_file=domain_prompts_file)
    
    # Extraer mÃ©tricas del texto (regex bÃ¡sico)
    text_data = analyzer.extract_text_metrics(text_data)
    print(f"   âœ“ MÃ©tricas extraÃ­das del texto (regex)")
    
    # Extraer metadata del documento
    print(f"\nðŸ“‹ Extrayendo metadata...")
    metadata = analyzer.extract_metadata(Path(original_path).name, text_data)
    
    # AnÃ¡lisis de texto con IA (si estÃ¡ habilitado)
    if analyzer.text_analysis_enabled:
        text_data = analyzer.analyze_text_with_ai(text_data)
    
    # Analizar imÃ¡genes
    chart_analysis = []
    if image_data:
        print(f"\n   â†’ Analizando {len(image_data)} grÃ¡ficos/tablas con IA...")
        chart_analysis = analyzer.analyze_all_images(image_data)
        print(f"   âœ“ {len(chart_analysis)} visualizaciones analizadas")
    
    # Paso 3: Crear anÃ¡lisis completo
    analysis = DocumentAnalysis(
        filename=Path(original_path).name,
        total_pages=len(text_data),
        metadata=metadata,
        text_data=text_data,
        image_data=image_data,
        chart_analysis=chart_analysis
    )
    
    # Paso 4: Guardar resultados
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    output_dir = Path(config['extraction']['output_dir']) / config['extraction']['data_dir']
    output_file = output_dir / f"{Path(file_path).stem}_analysis.ndjson"
    
    # Guardar en formato NDJSON (meta/claim/summary)
    ndjson_file = save_ndjson(analysis, output_file, file_path)
    
    print(f"\nðŸ’¾ Resultados guardados en: {ndjson_file}")
    
    # Crear resumen de insights (con filtros de relevancia y tipo)
    relevance_threshold = config.get('analysis', {}).get('relevance_threshold', DEFAULT_RELEVANCE_THRESHOLD)
    insight_filter = config.get('analysis', {}).get('insight_filter', DEFAULT_INSIGHT_FILTER)
    show_classification = config.get('analysis', {}).get('show_insight_classification', True)
    
    insights_file = create_insights_summary(
        analysis, 
        ndjson_file,  # Cambiado de output_file a ndjson_file
        relevance_threshold,
        insight_filter,
        show_classification
    )
    print(f"ðŸ“„ Resumen de insights: {insights_file}")
    
    # Exportar a DOCX si estÃ¡ habilitado (por config o argumento CLI)
    export_docx_enabled = export_docx or config.get('analysis', {}).get('export_docx', False)
    if export_docx_enabled and DOCX_EXPORT_AVAILABLE:
        try:
            docx_file = export_to_docx(ndjson_file)
            print(f"ðŸ“‘ Tabla Word exportada: {docx_file}")
        except Exception as e:
            print(f"âš ï¸  Error exportando DOCX: {e}")
    elif export_docx_enabled and not DOCX_EXPORT_AVAILABLE:
        print("âš ï¸  ExportaciÃ³n DOCX no disponible. Instala: pip install python-docx")
    
    # Calcular estadÃ­sticas de relevancia y clasificaciÃ³n
    relevant_charts = [c for c in analysis.chart_analysis if c.relevance_score >= relevance_threshold]
    relevant_text_pages = [t for t in analysis.text_data if t.ai_analysis and t.ai_analysis.relevance_score >= relevance_threshold]
    
    # Contar hallazgos vs hipÃ³tesis vs notas metodolÃ³gicas
    total_findings = 0
    total_hypotheses = 0
    total_methodological = 0
    for chart in analysis.chart_analysis:
        for insight in chart.insights:
            if insight.classification == "finding":
                total_findings += 1
            elif insight.classification == "hypothesis":
                total_hypotheses += 1
            else:
                total_methodological += 1
    for td in analysis.text_data:
        if td.ai_analysis:
            for insight in td.ai_analysis.insights:
                if insight.classification == "finding":
                    total_findings += 1
                elif insight.classification == "hypothesis":
                    total_hypotheses += 1
                else:
                    total_methodological += 1
    
    # Mostrar resumen
    print(f"\n{'='*60}")
    print("RESUMEN")
    print(f"{'='*60}")
    print(f"Total pÃ¡ginas: {analysis.total_pages}")
    print(f"ImÃ¡genes extraÃ­das: {len(analysis.image_data)}")
    print(f"GrÃ¡ficos analizados: {len(analysis.chart_analysis)} ({len(relevant_charts)} relevantes)")
    if analyzer.text_analysis_enabled:
        total_text_analyzed = len([t for t in analysis.text_data if t.ai_analysis])
        print(f"PÃ¡ginas de texto analizadas: {total_text_analyzed} ({len(relevant_text_pages)} relevantes)")
    print(f"Umbral de relevancia: {relevance_threshold}")
    print(f"ðŸ“Š Hallazgos: {total_findings} | ðŸ’¡ HipÃ³tesis: {total_hypotheses} | ðŸ“ Notas: {total_methodological}")
    
    if analysis.chart_analysis:
        print(f"\nPrimeros insights encontrados:")
        for i, chart in enumerate(analysis.chart_analysis[:3], 1):
            print(f"\n  {i}. {chart.title or 'Sin tÃ­tulo'}")
            print(f"     Tipo: {chart.chart_data.type}")
            if chart.insights:
                first_insight = chart.insights[0]
                labels = {"finding": "Hallazgo", "hypothesis": "HipÃ³tesis", "methodological_note": "Nota"}
                classification = labels.get(first_insight.classification, "HipÃ³tesis")
                print(f"     [{classification}]: {first_insight.text[:80]}...")
    
    return analysis


def main():
    parser = argparse.ArgumentParser(
        description="Procesa documentos PDF/PPT extrayendo texto, imÃ¡genes y analizando grÃ¡ficos",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ConfiguraciÃ³n de API Keys:
  OpciÃ³n 1 (Recomendada) - Variables de entorno:
    export ANTHROPIC_API_KEY='tu-key-anthropic'
    export OPENAI_API_KEY='tu-key-openai'
  
  OpciÃ³n 2 - Archivo config.json:
    {
      "analysis": {
        "provider": "anthropic",  // o "openai"
        "anthropic_api_key": "tu-key",
        "openai_api_key": "tu-key"
      }
    }

Ejemplos:
  # Uso bÃ¡sico (prompts genÃ©ricos)
  python main.py documento.pdf
  
  # Con prompts especÃ­ficos de dominio
  python main.py documento.pdf --domain-prompts afp_chile
  
  # Con configuraciÃ³n personalizada
  python main.py documento.pdf --config config_custom.json --domain-prompts afp_chile
        """
    )
    parser.add_argument(
        "file",
        nargs='+',
        help="Ruta(s) al archivo(s) PDF o PPTX a procesar. Acepta mÃºltiples archivos o wildcards (*.pdf)"
    )
    parser.add_argument(
        "--config",
        default="config.json",
        help="Ruta al archivo de configuraciÃ³n (default: config.json)"
    )
    parser.add_argument(
        "--domain-prompts",
        dest="domain_prompts",
        help="Nombre del archivo de prompts especÃ­ficos del dominio (ej: afp_chile). Se busca en prompts/domains/"
    )
    parser.add_argument(
        "--export-docx",
        action="store_true",
        help="Exportar tambiÃ©n a tabla Word (.docx)"
    )
    
    args = parser.parse_args()
    
    # Validar configuraciÃ³n
    if not Path(args.config).exists():
        print(f"âŒ Error: El archivo de configuraciÃ³n '{args.config}' no existe")
        return
    
    # Expandir wildcards (*.pdf) si es necesario
    all_files = []
    for pattern in args.file:
        if '*' in pattern or '?' in pattern:
            # Es un wildcard, expandir
            expanded = glob.glob(pattern)
            if expanded:
                all_files.extend(expanded)
            else:
                print(f"âš ï¸  No se encontraron archivos que coincidan con: {pattern}")
        else:
            # Es un archivo especÃ­fico
            all_files.append(pattern)
    
    if not all_files:
        print("âŒ Error: No se especificaron archivos vÃ¡lidos para procesar")
        return
    
    # Procesar cada archivo
    total_files = len(all_files)
    successful = 0
    failed = 0
    
    for idx, file_path in enumerate(all_files, 1):
        if not Path(file_path).exists():
            print(f"\nâš ï¸  [{idx}/{total_files}] Archivo no encontrado: {file_path}")
            failed += 1
            continue
        
        try:
            if total_files > 1:
                print(f"\n{'='*60}")
                print(f"Procesando archivo {idx}/{total_files}")
                print(f"{'='*60}")
            
            analysis = process_document(
                file_path, 
                args.config, 
                domain_prompts_file=args.domain_prompts,
                export_docx=args.export_docx
            )
            successful += 1
            
        except Exception as e:
            print(f"\nâŒ Error procesando {file_path}: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
            continue
    
    # Resumen final
    print(f"\n{'='*60}")
    print(f"RESUMEN FINAL: {successful} exitosos | {failed} fallidos de {total_files} archivos")
    print(f"{'='*60}")
    
    if failed > 0:
        raise SystemExit(1)


if __name__ == "__main__":
    main()
